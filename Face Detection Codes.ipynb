{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9766e2e7-5e17-451a-be7a-bad34054b823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.20.0-cp313-cp313-win_amd64.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: opencv-python in c:\\users\\suraj garole\\conda\\lib\\site-packages (4.12.0.88)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (25.9.23)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google_pasta>=0.1.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt_einsum>=2.3.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (24.2)\n",
      "Requirement already satisfied: protobuf>=5.28.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (5.29.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (1.75.0)\n",
      "Requirement already satisfied: tensorboard~=2.20.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (2.20.0)\n",
      "Requirement already satisfied: keras>=3.10.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (3.11.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (2.1.3)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (3.12.1)\n",
      "Requirement already satisfied: ml_dtypes<1.0.0,>=0.5.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: pillow in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in c:\\users\\suraj garole\\conda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
      "Requirement already satisfied: namex in c:\\users\\suraj garole\\conda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in c:\\users\\suraj garole\\conda\\lib\\site-packages (from keras>=3.10.0->tensorflow) (0.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
      "Using cached tensorflow-2.20.0-cp313-cp313-win_amd64.whl (332.0 MB)\n",
      "Installing collected packages: tensorflow\n",
      "Successfully installed tensorflow-2.20.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61dc06c2-f73a-47a9-b5ad-6c13be04a26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32717 files belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suraj Garole\\Conda\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1023/1023\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2074s\u001b[0m 2s/step - accuracy: 0.6102 - loss: 0.6641\n",
      "Epoch 2/5\n",
      "\u001b[1m1023/1023\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m659s\u001b[0m 643ms/step - accuracy: 0.6113 - loss: 0.6485 \n",
      "Epoch 3/5\n",
      "\u001b[1m1023/1023\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m676s\u001b[0m 637ms/step - accuracy: 0.6114 - loss: 0.6369 \n",
      "Epoch 4/5\n",
      "\u001b[1m1023/1023\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m603s\u001b[0m 589ms/step - accuracy: 0.6112 - loss: 0.6257\n",
      "Epoch 5/5\n",
      "\u001b[1m1023/1023\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m549s\u001b[0m 537ms/step - accuracy: 0.6286 - loss: 0.6121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[âœ…] Training done. Model saved as 'liveness_model.h5'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# âœ… Path should directly contain 'REAL' and 'FAKE' folders\n",
    "dataset_path = r\"C:\\archive (1)\\real_vs_fake\\real-vs-fake\"\n",
    "\n",
    "IMG_SIZE = 64\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    dataset_path,\n",
    "    labels=\"inferred\",\n",
    "    label_mode=\"binary\",\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Normalize images\n",
    "train_dataset = train_dataset.map(lambda x, y: (x / 255.0, y))\n",
    "\n",
    "# Build the model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile and train\n",
    "model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_dataset, epochs=EPOCHS)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"liveness_model.h5\")\n",
    "print(\"[âœ…] Training done. Model saved as 'liveness_model.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d50a4332-d327-471f-bb8d-9ab8f938015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Webcam started. Show a face to detect real or fake. Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# âœ… Load your pre-trained model\n",
    "model_path = \"liveness_model.h5\"\n",
    "try:\n",
    "    model = load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Could not load model from '{model_path}'.\\nReason: {e}\")\n",
    "    raise SystemExit\n",
    "\n",
    "# âœ… Set image size as per training config\n",
    "IMG_SIZE = 64\n",
    "\n",
    "# âœ… Load Haar Cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# âœ… Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"[ERROR] Could not access webcam.\")\n",
    "    raise SystemExit\n",
    "\n",
    "print(\"[INFO] Webcam started. Show a face to detect real or fake. Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"[ERROR] Failed to read frame from webcam.\")\n",
    "        break\n",
    "\n",
    "    # ðŸ”¹ Convert frame to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # ðŸ”¹ Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)  # Convert BGR â†’ RGB\n",
    "        face = cv2.resize(face, (IMG_SIZE, IMG_SIZE))\n",
    "        face = img_to_array(face)\n",
    "        face = np.expand_dims(face, axis=0)\n",
    "        face = face / 255.0  # Normalize\n",
    "\n",
    "        # ðŸ”¹ Model prediction\n",
    "        prediction = model.predict(face, verbose=0)[0][0]\n",
    "        label = \"Real\" if prediction >= 0.5 else \"Fake\"\n",
    "        color = (0, 255, 0) if label == \"Real\" else (0, 0, 255)\n",
    "        confidence = prediction * 100 if label == \"Real\" else (1 - prediction) * 100\n",
    "        text = f\"{label} ({confidence:.2f}%)\"\n",
    "\n",
    "        # ðŸ”¹ Draw rectangle & label\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(frame, text, (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    # ðŸ”¹ Show output\n",
    "    cv2.imshow(\"Liveness Detection\", frame)\n",
    "\n",
    "    # ðŸ”¹ Exit on pressing 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# âœ… Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82a236e9-88e3-4514-a3b6-8c7a28a2e98e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement mediapipe (from versions: none)\n",
      "ERROR: No matching distribution found for mediapipe\n"
     ]
    }
   ],
   "source": [
    "!pip install mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5287c537-2d2d-445a-9bce-24ba7b8c2b30",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deque\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "import time\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "EAR_THRESHOLD = 0.21\n",
    "CONSEC_FRAMES = 3\n",
    "MIN_BLINK_RATE = 5  # per minute\n",
    "IMG_SIZE = 64\n",
    "PREDICTION_HISTORY_SIZE = 10\n",
    "\n",
    "model = load_model(\"liveness_model.h5\")\n",
    "\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(refine_landmarks=True, max_num_faces=1)\n",
    "LEFT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "RIGHT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "\n",
    "blink_counter = 0\n",
    "total_blinks = 0\n",
    "start_time = time.time()\n",
    "\n",
    "prediction_queue = deque(maxlen=PREDICTION_HISTORY_SIZE)\n",
    "last_label = \"Analyzing...\"\n",
    "last_confidence = 0.0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"[INFO] Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results = face_mesh.process(rgb)\n",
    "\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            left_eye = [(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in LEFT_EYE]\n",
    "            right_eye = [(int(face_landmarks.landmark[i].x * w), int(face_landmarks.landmark[i].y * h)) for i in RIGHT_EYE]\n",
    "\n",
    "            for pt in left_eye + right_eye:\n",
    "                cv2.circle(frame, pt, 2, (0, 255, 0), -1)\n",
    "\n",
    "            ear = (eye_aspect_ratio(left_eye) + eye_aspect_ratio(right_eye)) / 2.0\n",
    "            if ear < EAR_THRESHOLD:\n",
    "                blink_counter += 1\n",
    "            else:\n",
    "                if blink_counter >= CONSEC_FRAMES:\n",
    "                    total_blinks += 1\n",
    "                blink_counter = 0\n",
    "\n",
    "            x_vals = [int(p.x * w) for p in face_landmarks.landmark]\n",
    "            y_vals = [int(p.y * h) for p in face_landmarks.landmark]\n",
    "            x_min, x_max = min(x_vals), max(x_vals)\n",
    "            y_min, y_max = min(y_vals), max(y_vals)\n",
    "\n",
    "            if x_max - x_min > 0 and y_max - y_min > 0:\n",
    "                face_crop = frame[y_min:y_max, x_min:x_max]\n",
    "                try:\n",
    "                    face_crop = cv2.resize(face_crop, (IMG_SIZE, IMG_SIZE))\n",
    "                    face_crop = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n",
    "                    face_crop = img_to_array(face_crop)\n",
    "                    face_crop = np.expand_dims(face_crop, axis=0) / 255.0\n",
    "                    prediction = model.predict(face_crop, verbose=0)[0][0]\n",
    "                    prediction_queue.append(prediction)\n",
    "\n",
    "                    if len(prediction_queue) == PREDICTION_HISTORY_SIZE:\n",
    "                        avg_pred = np.mean(prediction_queue)\n",
    "                        blink_rate = total_blinks / ((time.time() - start_time) / 60)\n",
    "                        if avg_pred >= 0.5 and blink_rate >= MIN_BLINK_RATE:\n",
    "                            last_label = \"Real\"\n",
    "                            last_confidence = avg_pred * 100\n",
    "                        else:\n",
    "                            last_label = \"Fake\"\n",
    "                            last_confidence = (1 - avg_pred) * 100\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"[WARN] Prediction error: {e}\")\n",
    "\n",
    "            color = (0, 255, 0) if last_label == \"Real\" else (0, 0, 255)\n",
    "            cv2.putText(frame, f\"{last_label} ({last_confidence:.2f}%)\", (10, 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "            cv2.putText(frame, f\"Blinks: {total_blinks}\", (10, 80),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "            blink_rate = total_blinks / ((time.time() - start_time) / 60)\n",
    "            cv2.putText(frame, f\"Blink Rate: {blink_rate:.2f}/min\", (10, 110),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Stable Liveness Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e79d0d2-f5ea-429c-957d-6469c9179cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Suraj Garole\\Conda\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f4dffb7-505e-4309-a476-ea60316a0ecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmediapipe\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize Mediapipe Hands and Drawing modules\u001b[39;00m\n\u001b[0;32m      5\u001b[0m mp_hands \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39msolutions\u001b[38;5;241m.\u001b[39mhands\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize Mediapipe Hands and Drawing modules\n",
    "mp_hands = mp.solutions.hands\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_hands.Hands(\n",
    "    max_num_hands=2,\n",
    "    min_detection_confidence=0.7,\n",
    "    min_tracking_confidence=0.5\n",
    ") as hands:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip the frame horizontally for a mirror effect\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Process the frame to detect hands\n",
    "        results = hands.process(frame_rgb)\n",
    "\n",
    "        # Draw hand landmarks\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS\n",
    "                )\n",
    "\n",
    "        # Display the output\n",
    "        cv2.imshow(\"Hand Tracking\", frame)\n",
    "\n",
    "        # Press ESC to exit\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "358c3c74-d8b0-4d0b-bf1c-68a05321e746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Webcam started. Show a face to detect real or fake. Press 'q' to quit.\n",
      "[INFO] Webcam closed.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "\n",
    "# ---------------- Load Pre-trained Model ----------------\n",
    "model_path = \"liveness_model.h5\"  # Make sure this file exists in the same folder\n",
    "try:\n",
    "    model = load_model(model_path)\n",
    "except Exception as e:\n",
    "    print(f\"[ERROR] Could not load model from '{model_path}'.\\nReason: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "IMG_SIZE = 64  # Image size used during training\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# ---------------- Initialize Webcam ----------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"[ERROR] Could not access webcam.\")\n",
    "    exit()\n",
    "\n",
    "print(\"[INFO] Webcam started. Show a face to detect real or fake. Press 'q' to quit.\")\n",
    "\n",
    "# ---------------- Main Loop ----------------\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"[ERROR] Failed to read frame from webcam.\")\n",
    "        break\n",
    "\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Extract face region\n",
    "        face = frame[y:y+h, x:x+w]\n",
    "        face_rgb = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)\n",
    "        face_resized = cv2.resize(face_rgb, (IMG_SIZE, IMG_SIZE))\n",
    "        face_array = img_to_array(face_resized)\n",
    "        face_array = np.expand_dims(face_array, axis=0)\n",
    "        face_array = face_array / 255.0  # Normalize\n",
    "\n",
    "        # Predict\n",
    "        prediction = model.predict(face_array, verbose=0)[0][0]\n",
    "        label = \"Real\" if prediction >= 0.5 else \"Fake\"\n",
    "        color = (0, 255, 0) if label == \"Real\" else (0, 0, 255)\n",
    "        confidence = prediction * 100 if label == \"Real\" else (1 - prediction) * 100\n",
    "        text = f\"{label} ({confidence:.2f}%)\"\n",
    "\n",
    "        # Draw rectangle and label\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(frame, text, (x, y - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    # Show frame\n",
    "    cv2.imshow(\"Liveness Detection\", frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# ---------------- Cleanup ----------------\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"[INFO] Webcam closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "302de816-ba7a-45e8-9d75-1c2841cccd55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "# EAR calculation\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "EAR_THRESHOLD = 0.21\n",
    "CONSEC_FRAMES = 3\n",
    "MIN_BLINK_RATE = 5\n",
    "IMG_SIZE = 64\n",
    "PREDICTION_HISTORY_SIZE = 10\n",
    "\n",
    "model = load_model(\"liveness_model.h5\")\n",
    "\n",
    "# Haar cascades (already in OpenCV)\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "blink_counter = 0\n",
    "total_blinks = 0\n",
    "start_time = time.time()\n",
    "\n",
    "prediction_queue = deque(maxlen=PREDICTION_HISTORY_SIZE)\n",
    "last_label = \"Analyzing...\"\n",
    "last_confidence = 0.0\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"[INFO] Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_crop = frame[y:y+h, x:x+w]\n",
    "        cv2.rectangle(frame, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "\n",
    "        eyes = eye_cascade.detectMultiScale(gray[y:y+h, x:x+w])\n",
    "        if len(eyes) >= 2:  # at least two eyes detected\n",
    "            blink_counter += 1\n",
    "        else:\n",
    "            if blink_counter >= CONSEC_FRAMES:\n",
    "                total_blinks += 1\n",
    "            blink_counter = 0\n",
    "\n",
    "        try:\n",
    "            face_resized = cv2.resize(face_crop, (IMG_SIZE, IMG_SIZE))\n",
    "            face_resized = cv2.cvtColor(face_resized, cv2.COLOR_BGR2RGB)\n",
    "            face_resized = img_to_array(face_resized)\n",
    "            face_resized = np.expand_dims(face_resized, axis=0) / 255.0\n",
    "            prediction = model.predict(face_resized, verbose=0)[0][0]\n",
    "            prediction_queue.append(prediction)\n",
    "\n",
    "            if len(prediction_queue) == PREDICTION_HISTORY_SIZE:\n",
    "                avg_pred = np.mean(prediction_queue)\n",
    "                blink_rate = total_blinks / ((time.time() - start_time) / 60)\n",
    "                if avg_pred >= 0.5 and blink_rate >= MIN_BLINK_RATE:\n",
    "                    last_label = \"Real\"\n",
    "                    last_confidence = avg_pred * 100\n",
    "                else:\n",
    "                    last_label = \"Fake\"\n",
    "                    last_confidence = (1 - avg_pred) * 100\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Prediction error: {e}\")\n",
    "\n",
    "        color = (0, 255, 0) if last_label == \"Real\" else (0, 0, 255)\n",
    "        cv2.putText(frame, f\"{last_label} ({last_confidence:.2f}%)\", (10, 40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "        cv2.putText(frame, f\"Blinks: {total_blinks}\", (10, 80),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "        blink_rate = total_blinks / ((time.time() - start_time) / 60)\n",
    "        cv2.putText(frame, f\"Blink Rate: {blink_rate:.2f}/min\", (10, 110),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(\"Stable Liveness Detection (No Mediapipe)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78d00d49-8bc3-4afa-a34b-84a7f9cc5fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque, defaultdict\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "# Thresholds\n",
    "EAR_THRESHOLD = 0.21\n",
    "CONSEC_FRAMES = 3\n",
    "MIN_BLINK_RATE = 1\n",
    "STABLE_PREDICTION_WINDOW = 2.0\n",
    "IMG_SIZE = 64\n",
    "MAX_HISTORY = 10\n",
    "BLUR_THRESHOLD = 150.0\n",
    "TEXTURE_THRESHOLD = 15.0\n",
    "BRIGHTNESS_THRESHOLD = 60.0\n",
    "\n",
    "# EAR calculation\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "def calculate_blur(image):\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def calculate_texture(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.std(gray)\n",
    "\n",
    "def calculate_brightness(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    _, _, v = cv2.split(hsv)\n",
    "    return np.mean(v)\n",
    "\n",
    "# Load model\n",
    "model = load_model(\"liveness_model.h5\")\n",
    "\n",
    "# Haar cascades\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# Track data per face\n",
    "face_data = defaultdict(lambda: {\n",
    "    \"blink_counter\": 0,\n",
    "    \"total_blinks\": 0,\n",
    "    \"predictions\": deque(maxlen=MAX_HISTORY),\n",
    "    \"last_label\": \"Analyzing...\",\n",
    "    \"last_confidence\": 0.0,\n",
    "    \"last_update_time\": 0.0,\n",
    "    \"start_time\": time.time()\n",
    "})\n",
    "\n",
    "def update_prediction(face_id, pred, blink_rate, blur, texture, brightness, current_time):\n",
    "    data = face_data[face_id]\n",
    "    data[\"predictions\"].append(pred)\n",
    "    avg_pred = np.mean(data[\"predictions\"])\n",
    "\n",
    "    if current_time - data[\"last_update_time\"] > STABLE_PREDICTION_WINDOW:\n",
    "        is_real_model = avg_pred >= 0.5\n",
    "        is_real_blink = blink_rate >= MIN_BLINK_RATE\n",
    "        is_real_texture = texture > TEXTURE_THRESHOLD or (brightness > BRIGHTNESS_THRESHOLD and texture > TEXTURE_THRESHOLD * 0.75)\n",
    "        is_low_light = brightness < BRIGHTNESS_THRESHOLD\n",
    "\n",
    "        if is_real_model and (is_real_blink or is_real_texture):\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "            data[\"last_confidence\"] = avg_pred * 100\n",
    "        elif is_low_light and is_real_model and is_real_texture:\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "            data[\"last_confidence\"] = avg_pred * 100\n",
    "        else:\n",
    "            data[\"last_label\"] = \"Fake\"\n",
    "            data[\"last_confidence\"] = (1 - avg_pred) * 100\n",
    "\n",
    "        data[\"last_update_time\"] = current_time\n",
    "\n",
    "# Start video\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"[INFO] Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for idx, (x, y, w, h) in enumerate(faces):\n",
    "        face_id = f\"face_{idx}\"\n",
    "        face_crop = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Eye detection\n",
    "        eyes = eye_cascade.detectMultiScale(gray[y:y+h, x:x+w])\n",
    "        if len(eyes) >= 2:\n",
    "            face_data[face_id][\"blink_counter\"] += 1\n",
    "        else:\n",
    "            if face_data[face_id][\"blink_counter\"] >= CONSEC_FRAMES:\n",
    "                face_data[face_id][\"total_blinks\"] += 1\n",
    "            face_data[face_id][\"blink_counter\"] = 0\n",
    "\n",
    "        elapsed = time.time() - face_data[face_id][\"start_time\"]\n",
    "        blink_rate = face_data[face_id][\"total_blinks\"] / (elapsed / 60) if elapsed > 0 else 0\n",
    "\n",
    "        try:\n",
    "            # Model prediction\n",
    "            face_input = cv2.resize(face_crop, (IMG_SIZE, IMG_SIZE))\n",
    "            face_rgb = cv2.cvtColor(face_input, cv2.COLOR_BGR2RGB)\n",
    "            face_input = img_to_array(face_rgb)\n",
    "            face_input = np.expand_dims(face_input, axis=0) / 255.0\n",
    "            prediction = model.predict(face_input, verbose=0)[0][0]\n",
    "\n",
    "            # Quality checks\n",
    "            blur_val = calculate_blur(face_crop)\n",
    "            texture_val = calculate_texture(face_crop)\n",
    "            brightness_val = calculate_brightness(face_crop)\n",
    "\n",
    "            update_prediction(face_id, prediction, blink_rate, blur_val, texture_val, brightness_val, time.time())\n",
    "\n",
    "            # Draw\n",
    "            label = face_data[face_id][\"last_label\"]\n",
    "            confidence = face_data[face_id][\"last_confidence\"]\n",
    "            color = (0, 255, 0) if label == \"Real\" else (0, 0, 255)\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} ({confidence:.1f}%)\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, f\"Blinks: {face_data[face_id]['total_blinks']}\", (x, y + h + 20),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "            cv2.putText(frame, f\"Rate: {blink_rate:.1f}/min\", (x, y + h + 40),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "            cv2.putText(frame, f\"Blur: {blur_val:.1f}\", (x, y + h + 60),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "            cv2.putText(frame, f\"Texture: {texture_val:.1f}\", (x, y + h + 80),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "            cv2.putText(frame, f\"Brightness: {brightness_val:.1f}\", (x, y + h + 100),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            continue\n",
    "\n",
    "    cv2.imshow(\"Multi-Face Liveness Detection (No Mediapipe)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f9df3556-5c7c-48e2-9a82-97d0ba485763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque, defaultdict\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "# Parameters\n",
    "EAR_THRESHOLD = 0.21\n",
    "CONSEC_FRAMES = 3\n",
    "MIN_BLINK_RATE = 2\n",
    "STABLE_PREDICTION_WINDOW = 2.0\n",
    "IMG_SIZE = 64\n",
    "MAX_HISTORY = 10\n",
    "BLUR_THRESHOLD = 150.0\n",
    "TEXTURE_THRESHOLD = 10.0\n",
    "BRIGHTNESS_THRESHOLD = 50.0\n",
    "\n",
    "# Load liveness model\n",
    "model = load_model(\"liveness_model.h5\")\n",
    "\n",
    "# Load OpenCV face and eye detectors\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "# Data storage for multiple faces\n",
    "face_data = defaultdict(lambda: {\n",
    "    \"blink_counter\": 0,\n",
    "    \"total_blinks\": 0,\n",
    "    \"predictions\": deque(maxlen=MAX_HISTORY),\n",
    "    \"last_label\": \"Analyzing...\",\n",
    "    \"last_confidence\": 0.0,\n",
    "    \"last_update_time\": 0.0,\n",
    "    \"start_time\": time.time()\n",
    "})\n",
    "\n",
    "# Helper functions\n",
    "def eye_aspect_ratio(eye):\n",
    "    if len(eye) < 6:\n",
    "        return 0\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    if C == 0:\n",
    "        return 0\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "def calculate_blur(image):\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def calculate_texture(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.std(gray)\n",
    "\n",
    "def calculate_brightness(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    _, _, v = cv2.split(hsv)\n",
    "    return np.mean(v)\n",
    "\n",
    "def remap_confidence(label, confidence):\n",
    "    if label == \"Real\":\n",
    "        return 90 + (confidence / 100 * 10)\n",
    "    else:\n",
    "        return 60 + (confidence / 100 * 10)\n",
    "\n",
    "def update_prediction(face_id, pred, blink_rate, blur, texture, brightness, current_time):\n",
    "    data = face_data[face_id]\n",
    "    data[\"predictions\"].append(pred)\n",
    "    avg_pred = np.mean(data[\"predictions\"])\n",
    "\n",
    "    if current_time - data[\"last_update_time\"] > STABLE_PREDICTION_WINDOW:\n",
    "        is_real_model = avg_pred >= 0.5\n",
    "        is_real_blink = blink_rate >= MIN_BLINK_RATE\n",
    "        is_real_texture = texture > TEXTURE_THRESHOLD or (brightness > BRIGHTNESS_THRESHOLD and texture > TEXTURE_THRESHOLD * 0.75)\n",
    "        is_low_light = brightness < BRIGHTNESS_THRESHOLD\n",
    "\n",
    "        if is_real_blink and is_real_texture and not is_low_light:\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "        elif is_real_model and (is_real_blink or is_real_texture):\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "        elif is_low_light and is_real_model and is_real_texture:\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "        else:\n",
    "            data[\"last_label\"] = \"Fake\"\n",
    "\n",
    "        data[\"last_confidence_raw\"] = avg_pred * 100\n",
    "        data[\"last_confidence\"] = remap_confidence(data[\"last_label\"], avg_pred * 100)\n",
    "        data[\"last_update_time\"] = current_time\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"[INFO] Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "    for idx, (x, y, w, h) in enumerate(faces):\n",
    "        face_id = f\"face_{idx}\"\n",
    "        face_crop = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Eye detection inside face\n",
    "        roi_gray = gray[y:y+h, x:x+w]\n",
    "        eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "        eye_points = []\n",
    "        for (ex, ey, ew, eh) in eyes[:2]:  # Use max 2 eyes\n",
    "            eye_center = (ex + ew//2, ey + eh//2)\n",
    "            eye_points.append(eye_center)\n",
    "            cv2.rectangle(frame, (x+ex, y+ey), (x+ex+ew, y+ey+eh), (0,255,0), 1)\n",
    "\n",
    "        ear = eye_aspect_ratio(eye_points)\n",
    "        if ear < EAR_THRESHOLD:\n",
    "            face_data[face_id][\"blink_counter\"] += 1\n",
    "        else:\n",
    "            if face_data[face_id][\"blink_counter\"] >= CONSEC_FRAMES:\n",
    "                face_data[face_id][\"total_blinks\"] += 1\n",
    "            face_data[face_id][\"blink_counter\"] = 0\n",
    "\n",
    "        elapsed = time.time() - face_data[face_id][\"start_time\"]\n",
    "        blink_rate = face_data[face_id][\"total_blinks\"] / (elapsed / 60) if elapsed > 0 else 0\n",
    "\n",
    "        # Prepare face for liveness model\n",
    "        try:\n",
    "            face_input = cv2.resize(face_crop, (IMG_SIZE, IMG_SIZE))\n",
    "            face_input = cv2.cvtColor(face_input, cv2.COLOR_BGR2RGB)\n",
    "            face_input = img_to_array(face_input)\n",
    "            face_input = np.expand_dims(face_input, axis=0) / 255.0\n",
    "            prediction = model.predict(face_input, verbose=0)[0][0]\n",
    "\n",
    "            blur_val = calculate_blur(face_crop)\n",
    "            texture_val = calculate_texture(face_crop)\n",
    "            brightness_val = calculate_brightness(face_crop)\n",
    "\n",
    "            update_prediction(face_id, prediction, blink_rate, blur_val, texture_val, brightness_val, time.time())\n",
    "\n",
    "            label = face_data[face_id][\"last_label\"]\n",
    "            confidence = face_data[face_id][\"last_confidence\"]\n",
    "            color = (0, 255, 0) if label==\"Real\" else (0,0,255)\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "            cv2.putText(frame, f\"{label} ({confidence:.1f}%)\", (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "            cv2.putText(frame, f\"Blinks: {face_data[face_id]['total_blinks']}\", (x, y+h+20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,0), 1)\n",
    "            cv2.putText(frame, f\"Rate: {blink_rate:.1f}/min\", (x, y+h+40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,0), 1)\n",
    "            cv2.putText(frame, f\"Blur: {blur_val:.1f}\", (x, y+h+60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,0), 1)\n",
    "            cv2.putText(frame, f\"Texture: {texture_val:.1f}\", (x, y+h+80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,0), 1)\n",
    "            cv2.putText(frame, f\"Brightness: {brightness_val:.1f}\", (x, y+h+100), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255), 1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Error:\", e)\n",
    "            continue\n",
    "\n",
    "    cv2.imshow(\"Liveness Detection (OpenCV Only)\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87df2024-10bd-4630-b8a2-c7c025179ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque, defaultdict\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from scipy.spatial import distance as dist\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# ---------------- Parameters ----------------\n",
    "EAR_THRESHOLD = 0.21\n",
    "CONSEC_FRAMES = 3\n",
    "MIN_BLINK_RATE = 2\n",
    "STABLE_PREDICTION_WINDOW = 2.0\n",
    "IMG_SIZE = 64\n",
    "MAX_HISTORY = 10\n",
    "TEXTURE_THRESHOLD = 10.0\n",
    "BRIGHTNESS_THRESHOLD = 50.0\n",
    "\n",
    "# ---------------- Eye Aspect Ratio ----------------\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "# ---------------- Image Metrics ----------------\n",
    "def calculate_blur(image):\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def calculate_texture(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.std(gray)\n",
    "\n",
    "def calculate_brightness(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    _, _, v = cv2.split(hsv)\n",
    "    return np.mean(v)\n",
    "\n",
    "# ---------------- Load Liveness Model ----------------\n",
    "model = load_model(\"liveness_model.h5\")\n",
    "\n",
    "# ---------------- Download DNN Face Detector if not exist ----------------\n",
    "model_dir = r\"C:\\models\\face_detector\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "proto_path = os.path.join(model_dir, \"deploy.prototxt\")\n",
    "model_path = os.path.join(model_dir, \"res10_300x300_ssd_iter_140000.caffemodel\")\n",
    "\n",
    "if not os.path.isfile(proto_path):\n",
    "    print(\"[INFO] Downloading deploy.prototxt...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt\",\n",
    "        proto_path\n",
    "    )\n",
    "\n",
    "if not os.path.isfile(model_path):\n",
    "    print(\"[INFO] Downloading res10_300x300_ssd_iter_140000.caffemodel...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://github.com/opencv/opencv_3rdparty/raw/dnn_samples_face_detector_20170830/res10_300x300_ssd_iter_140000.caffemodel\",\n",
    "        model_path\n",
    "    )\n",
    "\n",
    "face_net = cv2.dnn.readNetFromCaffe(proto_path, model_path)\n",
    "\n",
    "# ---------------- Eye Cascade for Blink Detection ----------------\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "# ---------------- Face Data ----------------\n",
    "face_data = defaultdict(lambda: {\n",
    "    \"blink_counter\": 0,\n",
    "    \"total_blinks\": 0,\n",
    "    \"predictions\": deque(maxlen=MAX_HISTORY),\n",
    "    \"last_label\": \"Analyzing...\",\n",
    "    \"last_confidence\": 0.0,\n",
    "    \"last_update_time\": 0.0,\n",
    "    \"start_time\": time.time()\n",
    "})\n",
    "\n",
    "def remap_confidence(label, confidence):\n",
    "    if label == \"Real\":\n",
    "        return 90 + (confidence / 100 * 10)\n",
    "    else:\n",
    "        return 60 + (confidence / 100 * 10)\n",
    "\n",
    "def update_prediction(face_id, pred, blink_rate, blur, texture, brightness, current_time):\n",
    "    data = face_data[face_id]\n",
    "    data[\"predictions\"].append(pred)\n",
    "    avg_pred = np.mean(data[\"predictions\"])\n",
    "\n",
    "    if current_time - data[\"last_update_time\"] > STABLE_PREDICTION_WINDOW:\n",
    "        is_real_model = avg_pred >= 0.5\n",
    "        is_real_blink = blink_rate >= MIN_BLINK_RATE\n",
    "        is_real_texture = texture > TEXTURE_THRESHOLD or (brightness > BRIGHTNESS_THRESHOLD and texture > TEXTURE_THRESHOLD * 0.75)\n",
    "        is_low_light = brightness < BRIGHTNESS_THRESHOLD\n",
    "\n",
    "        if is_real_blink and is_real_texture and not is_low_light:\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "        elif is_real_model and (is_real_blink or is_real_texture):\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "        elif is_low_light and is_real_model and is_real_texture:\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "        else:\n",
    "            data[\"last_label\"] = \"Fake\"\n",
    "\n",
    "        data[\"last_confidence_raw\"] = avg_pred * 100\n",
    "        data[\"last_confidence\"] = remap_confidence(data[\"last_label\"], avg_pred * 100)\n",
    "        data[\"last_update_time\"] = current_time\n",
    "\n",
    "# ---------------- Webcam Loop ----------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"[INFO] Press 'q' to quit.\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    # ---------------- DNN Face Detection ----------------\n",
    "    blob = cv2.dnn.blobFromImage(cv2.resize(frame, (300, 300)), 1.0,\n",
    "                                 (300, 300), (104.0, 177.0, 123.0))\n",
    "    face_net.setInput(blob)\n",
    "    detections = face_net.forward()\n",
    "\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "        if confidence < 0.5:\n",
    "            continue\n",
    "\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (x_min, y_min, x_max, y_max) = box.astype(\"int\")\n",
    "        face_crop = frame[y_min:y_max, x_min:x_max]\n",
    "\n",
    "        if face_crop.size == 0:\n",
    "            continue\n",
    "\n",
    "        # ---------------- Liveness Prediction ----------------\n",
    "        face_input = cv2.resize(face_crop, (IMG_SIZE, IMG_SIZE))\n",
    "        face_rgb = cv2.cvtColor(face_input, cv2.COLOR_BGR2RGB)\n",
    "        face_input = img_to_array(face_rgb)\n",
    "        face_input = np.expand_dims(face_input, axis=0) / 255.0\n",
    "        pred = model.predict(face_input, verbose=0)[0][0]\n",
    "\n",
    "        blur_val = calculate_blur(face_crop)\n",
    "        texture_val = calculate_texture(face_crop)\n",
    "        brightness_val = calculate_brightness(face_crop)\n",
    "\n",
    "        face_id = f\"face_{i}\"\n",
    "        elapsed = time.time() - face_data[face_id][\"start_time\"]\n",
    "        blink_rate = face_data[face_id][\"total_blinks\"] / (elapsed / 60) if elapsed > 0 else 0\n",
    "\n",
    "        update_prediction(face_id, pred, blink_rate, blur_val, texture_val, brightness_val, time.time())\n",
    "\n",
    "        label = face_data[face_id][\"last_label\"]\n",
    "        conf = face_data[face_id][\"last_confidence\"]\n",
    "        color = (0, 255, 0) if label == \"Real\" else (0, 0, 255)\n",
    "\n",
    "        cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "        cv2.putText(frame, f\"{label} ({conf:.1f}%)\", (x_min, y_min - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        cv2.putText(frame, f\"Blur: {blur_val:.1f}\", (x_min, y_max + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        cv2.putText(frame, f\"Texture: {texture_val:.1f}\", (x_min, y_max + 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 0), 1)\n",
    "        cv2.putText(frame, f\"Brightness: {brightness_val:.1f}\", (x_min, y_max + 60), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "\n",
    "    cv2.imshow(\"Liveness Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa75cb55-8132-4d1c-bb3c-8afb10f9326c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from collections import deque, defaultdict\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "# ---------------- Parameters ----------------\n",
    "EAR_THRESHOLD = 0.21\n",
    "CONSEC_FRAMES = 3\n",
    "MIN_BLINK_RATE = 2\n",
    "STABLE_PREDICTION_WINDOW = 2.0\n",
    "IMG_SIZE = 64\n",
    "MAX_HISTORY = 10\n",
    "TEXTURE_THRESHOLD = 10.0\n",
    "BRIGHTNESS_THRESHOLD = 40.0  # more lenient for low light\n",
    "\n",
    "# ---------------- Functions ----------------\n",
    "def calculate_blur(image):\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def calculate_texture(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.std(gray)\n",
    "\n",
    "def calculate_brightness(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    _, _, v = cv2.split(hsv)\n",
    "    return np.mean(v)\n",
    "\n",
    "def remap_confidence(label, confidence):\n",
    "    if label == \"Real\":\n",
    "        return 90 + (confidence / 100 * 10)  # 90â€“100%\n",
    "    else:\n",
    "        return 60 + (confidence / 100 * 10)  # 60â€“70%\n",
    "\n",
    "def update_prediction(face_id, pred, blink_rate, texture, brightness, current_time):\n",
    "    data = face_data[face_id]\n",
    "    data[\"predictions\"].append(pred)\n",
    "    avg_pred = np.mean(data[\"predictions\"])\n",
    "\n",
    "    if current_time - data[\"last_update_time\"] > STABLE_PREDICTION_WINDOW:\n",
    "        is_real_model = avg_pred >= 0.5\n",
    "        is_real_blink = blink_rate >= MIN_BLINK_RATE\n",
    "        is_real_texture = texture > TEXTURE_THRESHOLD or (brightness > BRIGHTNESS_THRESHOLD and texture > TEXTURE_THRESHOLD * 0.75)\n",
    "        is_low_light = brightness < BRIGHTNESS_THRESHOLD\n",
    "\n",
    "        if is_real_blink and is_real_texture and not is_low_light:\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "        elif is_real_model and (is_real_blink or is_real_texture):\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "        elif is_low_light and is_real_model and is_real_texture:\n",
    "            data[\"last_label\"] = \"Real\"\n",
    "        else:\n",
    "            data[\"last_label\"] = \"Fake\"\n",
    "\n",
    "        data[\"last_confidence_raw\"] = avg_pred * 100\n",
    "        data[\"last_confidence\"] = remap_confidence(data[\"last_label\"], avg_pred * 100)\n",
    "        data[\"last_update_time\"] = current_time\n",
    "\n",
    "# ---------------- Load Models ----------------\n",
    "model = load_model(\"liveness_model.h5\")\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "# ---------------- Data ----------------\n",
    "face_data = defaultdict(lambda: {\n",
    "    \"blink_counter\": 0,\n",
    "    \"total_blinks\": 0,\n",
    "    \"predictions\": deque(maxlen=MAX_HISTORY),\n",
    "    \"last_label\": \"Analyzing...\",\n",
    "    \"last_confidence\": 0.0,\n",
    "    \"last_update_time\": 0.0,\n",
    "    \"start_time\": time.time()\n",
    "})\n",
    "\n",
    "# ---------------- Capture ----------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "print(\"[INFO] Press 'q' to quit.\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    for i, (x, y, w, h) in enumerate(faces):\n",
    "        face_crop = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # Prepare for model prediction\n",
    "        face_input = cv2.resize(face_crop, (IMG_SIZE, IMG_SIZE))\n",
    "        face_input = cv2.cvtColor(face_input, cv2.COLOR_BGR2RGB)\n",
    "        face_input = img_to_array(face_input)\n",
    "        face_input = np.expand_dims(face_input, axis=0) / 255.0\n",
    "        pred = model.predict(face_input, verbose=0)[0][0]\n",
    "\n",
    "        # Blink detection\n",
    "        gray_face = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n",
    "        eyes = eye_cascade.detectMultiScale(gray_face, scaleFactor=1.1, minNeighbors=5)\n",
    "        ear = len(eyes) / 2.0  # simple proxy for blink\n",
    "\n",
    "        face_id = f\"face_{i}\"\n",
    "        elapsed = time.time() - face_data[face_id][\"start_time\"]\n",
    "        blink_rate = face_data[face_id][\"total_blinks\"] / (elapsed / 60) if elapsed > 0 else 0\n",
    "\n",
    "        # Feature calculations\n",
    "        blur_val = calculate_blur(face_crop)\n",
    "        texture_val = calculate_texture(face_crop)\n",
    "        brightness_val = calculate_brightness(face_crop)\n",
    "\n",
    "        update_prediction(face_id, pred, blink_rate, texture_val, brightness_val, time.time())\n",
    "\n",
    "        label = face_data[face_id][\"last_label\"]\n",
    "        conf = face_data[face_id][\"last_confidence\"]\n",
    "        color = (0, 255, 0) if label == \"Real\" else (0, 0, 255)\n",
    "\n",
    "        # Display\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), color, 2)\n",
    "        cv2.putText(frame, f\"{label} ({conf:.1f}%)\", (x, y-10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)\n",
    "        cv2.putText(frame, f\"Blinks: {face_data[face_id]['total_blinks']}\", (x, y+h+20),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,0),1)\n",
    "        cv2.putText(frame, f\"Brightness: {brightness_val:.1f}\", (x, y+h+40),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,255,255),1)\n",
    "        cv2.putText(frame, f\"Texture: {texture_val:.1f}\", (x, y+h+60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255,255,255),1)\n",
    "\n",
    "    cv2.imshow(\"Offline Liveness Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcb5484-ceaf-4da1-b19f-6c97f43434ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
