{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108e8eaf-a2e5-4f55-bca4-af47caa6ddd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Using cached sounddevice-0.5.2-py3-none-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting librosa\n",
      "  Using cached librosa-0.11.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\suraj garole\\conda\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sounddevice) (1.17.1)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\suraj garole\\appdata\\roaming\\python\\python313\\site-packages (from librosa) (5.2.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Using cached pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Using cached soxr-1.0.0-cp312-abi3-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Collecting standard-sunau (from librosa)\n",
      "  Using cached standard_sunau-3.13.0-py3-none-any.whl.metadata (914 bytes)\n",
      "Requirement already satisfied: pycparser in c:\\users\\suraj garole\\conda\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: packaging in c:\\users\\suraj garole\\appdata\\roaming\\python\\python313\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\suraj garole\\appdata\\roaming\\python\\python313\\site-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\suraj garole\\conda\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\suraj garole\\conda\\lib\\site-packages (from standard-aifc->librosa) (0.2.2)\n",
      "Using cached sounddevice-0.5.2-py3-none-win_amd64.whl (363 kB)\n",
      "Using cached librosa-0.11.0-py3-none-any.whl (260 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Using cached soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Using cached soxr-1.0.0-cp312-abi3-win_amd64.whl (172 kB)\n",
      "Using cached standard_sunau-3.13.0-py3-none-any.whl (7.4 kB)\n",
      "Installing collected packages: standard-sunau, soxr, audioread, soundfile, sounddevice, pooch, librosa\n",
      "\n",
      "   ----- ---------------------------------- 1/7 [soxr]\n",
      "   ----------- ---------------------------- 2/7 [audioread]\n",
      "   ----------------- ---------------------- 3/7 [soundfile]\n",
      "   ---------------------- ----------------- 4/7 [sounddevice]\n",
      "   ---------------------------- ----------- 5/7 [pooch]\n",
      "   ---------------------------- ----------- 5/7 [pooch]\n",
      "   ---------------------------- ----------- 5/7 [pooch]\n",
      "   ---------------------------- ----------- 5/7 [pooch]\n",
      "   ---------------------------------- ----- 6/7 [librosa]\n",
      "   ---------------------------------- ----- 6/7 [librosa]\n",
      "   ---------------------------------- ----- 6/7 [librosa]\n",
      "   ---------------------------------- ----- 6/7 [librosa]\n",
      "   ---------------------------------- ----- 6/7 [librosa]\n",
      "   ---------------------------------- ----- 6/7 [librosa]\n",
      "   ---------------------------------- ----- 6/7 [librosa]\n",
      "   ---------------------------------- ----- 6/7 [librosa]\n",
      "   ---------------------------------------- 7/7 [librosa]\n",
      "\n",
      "Successfully installed audioread-3.0.1 librosa-0.11.0 pooch-1.8.2 sounddevice-0.5.2 soundfile-0.13.1 soxr-1.0.0 standard-sunau-3.13.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice librosa joblib numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5e37ac5-b0a6-46fb-a497-b2bd913c1d9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sounddevice in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: librosa in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: soundfile in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\suraj garole\\conda\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: scikit-learn>=1.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\suraj garole\\appdata\\roaming\\python\\python313\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\suraj garole\\conda\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: packaging in c:\\users\\suraj garole\\appdata\\roaming\\python\\python313\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\suraj garole\\appdata\\roaming\\python\\python313\\site-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn>=1.1.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\suraj garole\\conda\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\suraj garole\\conda\\lib\\site-packages (from standard-aifc->librosa) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice librosa joblib soundfile numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef5a6c77-8d17-4540-bb14-a13be1cff71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sounddevice in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.5.2)\n",
      "Requirement already satisfied: soundfile in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.13.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\suraj garole\\conda\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: librosa in c:\\users\\suraj garole\\conda\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\suraj garole\\conda\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from sounddevice) (1.17.1)\n",
      "Requirement already satisfied: audioread>=2.1.9 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (3.0.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (0.61.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.15.3)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\suraj garole\\appdata\\roaming\\python\\python313\\site-packages (from librosa) (5.2.1)\n",
      "Requirement already satisfied: pooch>=1.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.0.0)\n",
      "Requirement already satisfied: typing_extensions>=4.1.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Requirement already satisfied: lazy_loader>=0.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (1.0.3)\n",
      "Requirement already satisfied: standard-aifc in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: standard-sunau in c:\\users\\suraj garole\\conda\\lib\\site-packages (from librosa) (3.13.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\suraj garole\\conda\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Requirement already satisfied: packaging in c:\\users\\suraj garole\\appdata\\roaming\\python\\python313\\site-packages (from lazy_loader>=0.1->librosa) (25.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from numba>=0.51.0->librosa) (0.44.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\suraj garole\\appdata\\roaming\\python\\python313\\site-packages (from pooch>=1.1->librosa) (4.4.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\suraj garole\\conda\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
      "Requirement already satisfied: standard-chunk in c:\\users\\suraj garole\\conda\\lib\\site-packages (from standard-aifc->librosa) (3.13.0)\n",
      "Requirement already satisfied: audioop-lts in c:\\users\\suraj garole\\conda\\lib\\site-packages (from standard-aifc->librosa) (0.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sounddevice soundfile numpy librosa joblib scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95df7479-9cd9-45e1-8567-6229d7eb9dc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dummy voice model created.\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import os\n",
    "\n",
    "model_path = \"voice_liveness_model.pkl\"\n",
    "expected_features = 6960  # same as used in feature extraction\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    # Create a dummy model with random data\n",
    "    X_dummy = np.random.rand(10, expected_features)\n",
    "    y_dummy = np.random.randint(0, 2, 10)  # 0 = Fake, 1 = Real\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_dummy, y_dummy)\n",
    "    joblib.dump(model, model_path)\n",
    "    print(\"‚úÖ Dummy voice model created.\")\n",
    "else:\n",
    "    print(\"‚úÖ Model already exists.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eba67a6-d321-4cf8-bd11-6deb5340f32a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé§ Speak now (recording for 2 seconds)...\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "\n",
    "duration = 2  # seconds\n",
    "samplerate = 16000\n",
    "temp_audio_file = \"temp_voice.wav\"\n",
    "\n",
    "print(\"üé§ Speak now (recording for 2 seconds)...\")\n",
    "audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1)\n",
    "sd.wait()\n",
    "sf.write(temp_audio_file, audio, samplerate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0d586d-8e99-4418-a8ff-2ec6fc8e10d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "y, sr = librosa.load(temp_audio_file, sr=samplerate)\n",
    "mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "features = mfcc.flatten()\n",
    "\n",
    "# Pad or truncate to expected features\n",
    "if len(features) < expected_features:\n",
    "    features = np.pad(features, (0, expected_features - len(features)), mode='constant')\n",
    "elif len(features) > expected_features:\n",
    "    features = features[:expected_features]\n",
    "\n",
    "features = features.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdc9a316-6c3f-4a6d-934f-bebb3274c3fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîé Voice Detected: Fake (50.00%)\n"
     ]
    }
   ],
   "source": [
    "model = joblib.load(model_path)\n",
    "\n",
    "try:\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(features)[0]\n",
    "        fake_score = proba[0] * 100\n",
    "        real_score = proba[1] * 100\n",
    "    else:\n",
    "        pred = model.predict(features)[0]\n",
    "        real_score = 100 if pred == 1 else 0\n",
    "        fake_score = 100 - real_score\n",
    "\n",
    "    label = \"Real\" if real_score > fake_score else \"Fake\"\n",
    "    conf = max(real_score, fake_score)\n",
    "\n",
    "    print(f\"\\nüîé Voice Detected: {label} ({conf:.2f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error during prediction:\", e)\n",
    "\n",
    "# Clean up temporary audio file\n",
    "import os\n",
    "if os.path.exists(temp_audio_file):\n",
    "    os.remove(temp_audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f928a00-0651-4600-8b60-0eed538fd695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Voice model already exists.\n",
      "\n",
      "üé§ Speak now (recording for 2 seconds)...\n",
      "\n",
      "üîé Voice Detected: Real (53.00%)\n"
     ]
    }
   ],
   "source": [
    "# ------------------ Imports ------------------\n",
    "import os\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import librosa\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# ------------------ Parameters ------------------\n",
    "model_path = \"voice_liveness_model.pkl\"\n",
    "temp_audio_file = \"temp_voice.wav\"\n",
    "duration = 2          # seconds to record\n",
    "samplerate = 16000    # audio sample rate\n",
    "expected_features = 6960  # number of features model expects\n",
    "\n",
    "# ------------------ Create Dummy Model if Not Found ------------------\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"‚¨áÔ∏è Creating dummy voice model...\")\n",
    "    X_dummy = np.random.rand(10, expected_features)\n",
    "    y_dummy = np.random.randint(0, 2, 10)  # 0 = Fake, 1 = Real\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X_dummy, y_dummy)\n",
    "    joblib.dump(model, model_path)\n",
    "    print(\"‚úÖ Dummy voice model created.\")\n",
    "else:\n",
    "    print(\"‚úÖ Voice model already exists.\")\n",
    "\n",
    "# Load model\n",
    "model = joblib.load(model_path)\n",
    "\n",
    "# ------------------ Record Audio ------------------\n",
    "print(f\"\\nüé§ Speak now (recording for {duration} seconds)...\")\n",
    "audio = sd.rec(int(duration * samplerate), samplerate=samplerate, channels=1)\n",
    "sd.wait()\n",
    "sf.write(temp_audio_file, audio, samplerate)\n",
    "\n",
    "# ------------------ Extract Features ------------------\n",
    "y, sr = librosa.load(temp_audio_file, sr=samplerate)\n",
    "if len(y) < 0.3 * sr:\n",
    "    print(\"‚ö†Ô∏è Voice too short. Try speaking louder or longer.\")\n",
    "    os.remove(temp_audio_file)\n",
    "    exit()\n",
    "\n",
    "mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "features = mfcc.flatten()\n",
    "\n",
    "# Pad or truncate features\n",
    "if len(features) < expected_features:\n",
    "    features = np.pad(features, (0, expected_features - len(features)), mode='constant')\n",
    "elif len(features) > expected_features:\n",
    "    features = features[:expected_features]\n",
    "\n",
    "features = features.reshape(1, -1)\n",
    "\n",
    "# ------------------ Predict ------------------\n",
    "try:\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        proba = model.predict_proba(features)[0]\n",
    "        fake_score = proba[0] * 100\n",
    "        real_score = proba[1] * 100\n",
    "    else:\n",
    "        pred = model.predict(features)[0]\n",
    "        real_score = 100 if pred == 1 else 0\n",
    "        fake_score = 100 - real_score\n",
    "\n",
    "    label = \"Real\" if real_score > fake_score else \"Fake\"\n",
    "    conf = max(real_score, fake_score)\n",
    "    print(f\"\\nüîé Voice Detected: {label} ({conf:.2f}%)\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Error during prediction:\", e)\n",
    "\n",
    "# ------------------ Clean Up ------------------\n",
    "if os.path.exists(temp_audio_file):\n",
    "    os.remove(temp_audio_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13aeb835-09e4-44d6-a36b-4da30fec073d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Voice model loaded.\n",
      "\n",
      "üé§ Listening for voice (recording for 2 seconds)...\n",
      "üîé Voice Detected: Real (52.00%)\n",
      "\n",
      "üé§ Listening for voice (recording for 2 seconds)...\n",
      "üîé Voice Detected: Real (51.00%)\n",
      "\n",
      "üé§ Listening for voice (recording for 2 seconds)...\n",
      "üîé Voice Detected: Fake (50.00%)\n",
      "\n",
      "üé§ Listening for voice (recording for 2 seconds)...\n",
      "üîé Voice Detected: Real (52.00%)\n",
      "\n",
      "üé§ Listening for voice (recording for 2 seconds)...\n",
      "üîé Voice Detected: Real (52.00%)\n",
      "\n",
      "üé§ Listening for voice (recording for 2 seconds)...\n",
      "üîé Voice Detected: Fake (50.00%)\n",
      "\n",
      "üé§ Listening for voice (recording for 2 seconds)...\n",
      "üîé Voice Detected: Fake (50.00%)\n",
      "\n",
      "üé§ Listening for voice (recording for 2 seconds)...\n",
      "üîé Voice Detected: Real (51.00%)\n",
      "\n",
      "\n",
      "üõë Detection stopped by user.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import joblib\n",
    "import os\n",
    "import time\n",
    "\n",
    "model_path = \"voice_liveness_model.pkl\"\n",
    "temp_audio_file = \"temp_voice.wav\"\n",
    "record_duration = 2  # seconds of voice input\n",
    "gap_between_checks = 5  # seconds between detections\n",
    "samplerate = 16000\n",
    "expected_features = 6960  # Model input size\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"‚ùå Model file 'voice_liveness_model.pkl' not found.\")\n",
    "    exit()\n",
    "model = joblib.load(model_path)\n",
    "print(\"‚úÖ Voice model loaded.\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        print(\"üé§ Listening for voice (recording for 2 seconds)...\")\n",
    "        audio = sd.rec(int(record_duration * samplerate), samplerate=samplerate, channels=1)\n",
    "        sd.wait()\n",
    "        sf.write(temp_audio_file, audio, samplerate)\n",
    "\n",
    "        y, sr = librosa.load(temp_audio_file, sr=samplerate)\n",
    "        if len(y) < 0.3 * sr:\n",
    "            print(\"‚ö†Ô∏è Voice too short or silent. Skipping...\\n\")\n",
    "            time.sleep(gap_between_checks)\n",
    "            continue\n",
    "\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        features = mfcc.flatten()\n",
    "\n",
    "        if len(features) < expected_features:\n",
    "            features = np.pad(features, (0, expected_features - len(features)), mode='constant')\n",
    "        elif len(features) > expected_features:\n",
    "            features = features[:expected_features]\n",
    "\n",
    "        features = features.reshape(1, -1)\n",
    "\n",
    "        try:\n",
    "            if hasattr(model, \"predict_proba\"):\n",
    "                proba = model.predict_proba(features)[0]\n",
    "                fake_score = proba[0] * 100\n",
    "                real_score = proba[1] * 100\n",
    "            else:\n",
    "                pred = model.predict(features)[0]\n",
    "                real_score = 100 if pred == 1 else 0\n",
    "                fake_score = 100 - real_score\n",
    "\n",
    "            label = \"Real\" if real_score > fake_score else \"Fake\"\n",
    "            conf = max(real_score, fake_score)\n",
    "\n",
    "            print(f\"üîé Voice Detected: {label} ({conf:.2f}%)\\n\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"‚ùå Error during prediction:\", e)\n",
    "\n",
    "        if os.path.exists(temp_audio_file):\n",
    "            os.remove(temp_audio_file)\n",
    "\n",
    "        time.sleep(gap_between_checks)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nüõë Detection stopped by user.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6f4cd32-d36f-4ad0-adfd-de225e153aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Press 'q' to quit...\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n",
      "[Face Error] list index out of range\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import queue\n",
    "import librosa\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from scipy.spatial import distance as dist\n",
    "from joblib import load as joblib_load\n",
    "\n",
    "# ------------------ Models ------------------\n",
    "face_model = load_model(\"liveness_model.h5\")  # face liveness model\n",
    "voice_model = joblib_load(\"voice_liveness_model.pkl\")  # voice liveness model\n",
    "\n",
    "IMG_SIZE = 64\n",
    "EAR_THRESHOLD = 0.21\n",
    "CONSEC_FRAMES = 3\n",
    "TEXTURE_THRESHOLD = 5.0\n",
    "sr = 16000  # audio sample rate\n",
    "\n",
    "q_audio = queue.Queue()\n",
    "audio_buffer = []\n",
    "\n",
    "# ------------------ Load OpenCV face & eye detectors ------------------\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "blink_counter = 0\n",
    "total_blinks = 0\n",
    "start_time = time.time()\n",
    "\n",
    "# ------------------ Audio recording callback ------------------\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(\"[Audio Callback Warning]\", status)\n",
    "    q_audio.put(indata.copy())\n",
    "\n",
    "# ------------------ MFCC extraction ------------------\n",
    "def extract_mfcc(audio, sr):\n",
    "    try:\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "        if mfcc.shape[1] < 174:\n",
    "            mfcc = np.pad(mfcc, ((0, 0), (0, 174 - mfcc.shape[1])), mode='constant')\n",
    "        return mfcc[:, :174].flatten().reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        print(\"[MFCC Error]\", e)\n",
    "        return None\n",
    "\n",
    "# ------------------ Eye Aspect Ratio ------------------\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = dist.euclidean(eye[1], eye[5])\n",
    "    B = dist.euclidean(eye[2], eye[4])\n",
    "    C = dist.euclidean(eye[0], eye[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "# ------------------ Texture calculation ------------------\n",
    "def calculate_texture(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.std(gray)\n",
    "\n",
    "# ------------------ Start video & audio ------------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "stream = sd.InputStream(samplerate=sr, channels=1, callback=audio_callback)\n",
    "stream.start()\n",
    "\n",
    "print(\"[INFO] Press 'q' to quit...\")\n",
    "\n",
    "voice_label = \"Waiting...\"\n",
    "voice_confidence = 0.0\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n",
    "    \n",
    "    face_label = \"No Face\"\n",
    "    face_confidence = 0.0\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_crop = frame[y:y+h, x:x+w]\n",
    "        resized = cv2.resize(face_crop, (IMG_SIZE, IMG_SIZE))\n",
    "        face_input = img_to_array(cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)) / 255.0\n",
    "        face_input = np.expand_dims(face_input, axis=0)\n",
    "\n",
    "        # ------------------ Face prediction ------------------\n",
    "        try:\n",
    "            prediction = face_model.predict(face_input, verbose=0)[0][0]\n",
    "            texture = calculate_texture(face_crop)\n",
    "\n",
    "            # ------------------ Eye detection ------------------\n",
    "            eyes = eye_cascade.detectMultiScale(cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY))\n",
    "            ear = 1.0  # default\n",
    "            if len(eyes) >= 2:\n",
    "                # pick first two eyes\n",
    "                eye_pts = []\n",
    "                for ex, ey, ew, eh in eyes[:2]:\n",
    "                    eye_pts.append((ex, ey))\n",
    "                    eye_pts.append((ex+ew, ey+eh))\n",
    "                ear = eye_aspect_ratio(eye_pts)\n",
    "\n",
    "            if prediction > 0.5 and ear < EAR_THRESHOLD and texture > TEXTURE_THRESHOLD:\n",
    "                is_real = True\n",
    "            else:\n",
    "                is_real = prediction > 0.5 and texture > TEXTURE_THRESHOLD\n",
    "\n",
    "            face_label = \"Real\" if is_real else \"Fake\"\n",
    "            face_confidence = prediction * 100\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"[Face Error]\", e)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0) if face_label==\"Real\" else (0,0,255), 2)\n",
    "\n",
    "    # ------------------ Audio processing ------------------\n",
    "    if not q_audio.empty():\n",
    "        audio_chunk = q_audio.get().flatten()\n",
    "        audio_buffer.extend(audio_chunk.tolist())\n",
    "\n",
    "    if len(audio_buffer) >= sr:\n",
    "        audio_np = np.array(audio_buffer[:sr])\n",
    "        audio_buffer = audio_buffer[sr:]\n",
    "        mfcc = extract_mfcc(audio_np, sr)\n",
    "        if mfcc is not None:\n",
    "            try:\n",
    "                prediction = voice_model.predict(mfcc)[0]\n",
    "                voice_label = \"Real\" if prediction == 1 else \"Fake\"\n",
    "                voice_confidence = 100.0 if prediction == 1 else 0.0\n",
    "            except Exception as e:\n",
    "                voice_label = \"Model Error\"\n",
    "                voice_confidence = 0.0\n",
    "                print(\"[Voice Model Error]\", e)\n",
    "\n",
    "    # ------------------ Display ------------------\n",
    "    cv2.putText(frame, f\"Face: {face_label} ({face_confidence:.1f}%)\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0) if face_label==\"Real\" else (0,0,255), 2)\n",
    "    cv2.putText(frame, f\"Voice: {voice_label} ({voice_confidence:.1f}%)\", (10, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0) if voice_label==\"Real\" else (0,0,255), 2)\n",
    "\n",
    "    cv2.imshow(\"Face & Voice Liveness Detection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "stream.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89d41444-010c-435a-bd60-9085391ae925",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Press 'q' to quit...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import queue\n",
    "import librosa\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from scipy.spatial import distance as dist\n",
    "from joblib import load as joblib_load\n",
    "\n",
    "# ---------------- Models ----------------\n",
    "face_model = load_model(\"liveness_model.h5\")\n",
    "voice_model = joblib_load(\"voice_liveness_model.pkl\")\n",
    "\n",
    "IMG_SIZE = 64\n",
    "EAR_THRESHOLD = 0.21\n",
    "CONSEC_FRAMES = 3\n",
    "TEXTURE_THRESHOLD = 5.0\n",
    "BRIGHTNESS_THRESHOLD = 50.0\n",
    "sr = 16000\n",
    "\n",
    "q_audio = queue.Queue()\n",
    "audio_buffer = []\n",
    "\n",
    "# ---------------- OpenCV cascades ----------------\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "blink_counter = 0\n",
    "total_blinks = 0\n",
    "start_time = time.time()\n",
    "\n",
    "voice_label = \"Waiting...\"\n",
    "voice_confidence = 0.0\n",
    "\n",
    "# ---------------- Audio callback ----------------\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(\"[Audio Warning]\", status)\n",
    "    q_audio.put(indata.copy())\n",
    "\n",
    "# ---------------- MFCC extraction ----------------\n",
    "def extract_mfcc(audio, sr):\n",
    "    try:\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "        if mfcc.shape[1] < 174:\n",
    "            mfcc = np.pad(mfcc, ((0, 0), (0, 174 - mfcc.shape[1])), mode='constant')\n",
    "        return mfcc[:, :174].flatten().reshape(1, -1)\n",
    "    except Exception as e:\n",
    "        print(\"[MFCC Error]\", e)\n",
    "        return None\n",
    "\n",
    "# ---------------- Eye Aspect Ratio ----------------\n",
    "def eye_aspect_ratio(eye_pts):\n",
    "    if len(eye_pts) < 6:\n",
    "        return 1.0\n",
    "    A = dist.euclidean(eye_pts[1], eye_pts[5])\n",
    "    B = dist.euclidean(eye_pts[2], eye_pts[4])\n",
    "    C = dist.euclidean(eye_pts[0], eye_pts[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "# ---------------- Texture, Blur, Brightness ----------------\n",
    "def calculate_texture(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.std(gray)\n",
    "\n",
    "def calculate_blur(image):\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def calculate_brightness(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    _, _, v = cv2.split(hsv)\n",
    "    return np.mean(v)\n",
    "\n",
    "# ---------------- Video & Audio ----------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "stream = sd.InputStream(samplerate=sr, channels=1, callback=audio_callback)\n",
    "stream.start()\n",
    "\n",
    "print(\"[INFO] Press 'q' to quit...\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(60, 60))\n",
    "\n",
    "    face_label = \"No Face\"\n",
    "    face_confidence = 0.0\n",
    "    blink_rate = 0\n",
    "    blur_val = 0\n",
    "    texture_val = 0\n",
    "    brightness_val = 0\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_crop = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # ------------- Face Liveness ----------------\n",
    "        try:\n",
    "            resized = cv2.resize(face_crop, (IMG_SIZE, IMG_SIZE))\n",
    "            face_input = img_to_array(cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)) / 255.0\n",
    "            face_input = np.expand_dims(face_input, axis=0)\n",
    "            prediction = face_model.predict(face_input, verbose=0)[0][0]\n",
    "\n",
    "            texture_val = calculate_texture(face_crop)\n",
    "            blur_val = calculate_blur(face_crop)\n",
    "            brightness_val = calculate_brightness(face_crop)\n",
    "\n",
    "            # ------------- Eye Blink Detection ------------\n",
    "            eyes = eye_cascade.detectMultiScale(cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY))\n",
    "            ear = 1.0\n",
    "            if len(eyes) >= 2:\n",
    "                eye_pts = []\n",
    "                for ex, ey, ew, eh in eyes[:2]:\n",
    "                    eye_pts.append((ex, ey))\n",
    "                    eye_pts.append((ex+ew, ey+eh))\n",
    "                ear = eye_aspect_ratio(eye_pts)\n",
    "\n",
    "            if ear < EAR_THRESHOLD:\n",
    "                blink_counter += 1\n",
    "            else:\n",
    "                if blink_counter >= CONSEC_FRAMES:\n",
    "                    total_blinks += 1\n",
    "                blink_counter = 0\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            blink_rate = total_blinks / (elapsed / 60) if elapsed > 0 else 0\n",
    "\n",
    "            # ------------- Final Face Decision -------------\n",
    "            is_real = prediction > 0.5 and blink_rate >= 1 and texture_val > TEXTURE_THRESHOLD\n",
    "            face_label = \"Real\" if is_real else \"Fake\"\n",
    "            face_confidence = prediction * 100\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"[Face Error]\", e)\n",
    "\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0) if face_label==\"Real\" else (0,0,255), 2)\n",
    "\n",
    "    # ---------------- Audio Processing ----------------\n",
    "    if not q_audio.empty():\n",
    "        audio_chunk = q_audio.get().flatten()\n",
    "        audio_buffer.extend(audio_chunk.tolist())\n",
    "\n",
    "    if len(audio_buffer) >= sr:\n",
    "        audio_np = np.array(audio_buffer[:sr])\n",
    "        audio_buffer = audio_buffer[sr:]\n",
    "        mfcc = extract_mfcc(audio_np, sr)\n",
    "        if mfcc is not None:\n",
    "            try:\n",
    "                prediction = voice_model.predict(mfcc)[0]\n",
    "                voice_label = \"Real\" if prediction == 1 else \"Fake\"\n",
    "                voice_confidence = 100.0 if prediction == 1 else 0.0\n",
    "            except Exception as e:\n",
    "                voice_label = \"Model Error\"\n",
    "                voice_confidence = 0.0\n",
    "                print(\"[Voice Model Error]\", e)\n",
    "\n",
    "    # ---------------- Display Info ----------------\n",
    "    cv2.putText(frame, f\"Face: {face_label} ({face_confidence:.1f}%)\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0) if face_label==\"Real\" else (0,0,255), 2)\n",
    "    cv2.putText(frame, f\"Voice: {voice_label} ({voice_confidence:.1f}%)\", (10, 60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0) if voice_label==\"Real\" else (0,0,255), 2)\n",
    "    cv2.putText(frame, f\"Blinks: {total_blinks}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 1)\n",
    "    cv2.putText(frame, f\"Blink Rate: {blink_rate:.2f}/min\", (10, 110), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 1)\n",
    "    cv2.putText(frame, f\"Blur: {blur_val:.2f}\", (10, 130), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 1)\n",
    "    cv2.putText(frame, f\"Texture: {texture_val:.2f}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 1)\n",
    "    cv2.putText(frame, f\"Brightness: {brightness_val:.2f}\", (10, 170), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 1)\n",
    "\n",
    "    cv2.imshow(\"Face & Voice Liveness Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "stream.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a77a43b-cd64-44c5-ad26-9742886b5472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Press 'q' to quit...\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "import sounddevice as sd\n",
    "import queue\n",
    "import librosa\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from scipy.spatial import distance as dist\n",
    "from joblib import load as joblib_load\n",
    "\n",
    "# ---------------- Models ----------------\n",
    "face_model = load_model(\"liveness_model.h5\")\n",
    "voice_model = joblib_load(\"voice_liveness_model.pkl\")\n",
    "\n",
    "IMG_SIZE = 64\n",
    "EAR_THRESHOLD = 0.21\n",
    "CONSEC_FRAMES = 3\n",
    "TEXTURE_THRESHOLD = 5.0\n",
    "VOICE_CONFIDENCE_THRESHOLD = 0.5  # for voice prediction\n",
    "sr = 16000\n",
    "\n",
    "q_audio = queue.Queue()\n",
    "audio_buffer = []\n",
    "\n",
    "# ---------------- OpenCV cascades ----------------\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_eye.xml\")\n",
    "\n",
    "blink_counter = 0\n",
    "total_blinks = 0\n",
    "start_time = time.time()\n",
    "\n",
    "voice_label = \"Waiting...\"\n",
    "voice_confidence = 0.0\n",
    "\n",
    "# ---------------- Audio callback ----------------\n",
    "def audio_callback(indata, frames, time_info, status):\n",
    "    if status:\n",
    "        print(\"[Audio Warning]\", status)\n",
    "    q_audio.put(indata.copy())\n",
    "\n",
    "# ---------------- MFCC extraction ----------------\n",
    "def extract_mfcc(audio, sr):\n",
    "    try:\n",
    "        energy = np.sum(audio ** 2)\n",
    "        if energy < 0.001:\n",
    "            return None  # ignore silence\n",
    "        mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "        if mfcc.shape[1] < 174:\n",
    "            mfcc = np.pad(mfcc, ((0,0),(0,174-mfcc.shape[1])), mode='constant')\n",
    "        return mfcc[:, :174].flatten().reshape(1,-1)\n",
    "    except Exception as e:\n",
    "        print(\"[MFCC Error]\", e)\n",
    "        return None\n",
    "\n",
    "# ---------------- Eye Aspect Ratio ----------------\n",
    "def eye_aspect_ratio(eye_pts):\n",
    "    if len(eye_pts) < 6:\n",
    "        return 1.0\n",
    "    A = dist.euclidean(eye_pts[1], eye_pts[5])\n",
    "    B = dist.euclidean(eye_pts[2], eye_pts[4])\n",
    "    C = dist.euclidean(eye_pts[0], eye_pts[3])\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "# ---------------- Texture, Blur, Brightness ----------------\n",
    "def calculate_texture(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    return np.std(gray)\n",
    "\n",
    "def calculate_blur(image):\n",
    "    return cv2.Laplacian(image, cv2.CV_64F).var()\n",
    "\n",
    "def calculate_brightness(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "    _, _, v = cv2.split(hsv)\n",
    "    return np.mean(v)\n",
    "\n",
    "# ---------------- Video & Audio ----------------\n",
    "cap = cv2.VideoCapture(0)\n",
    "stream = sd.InputStream(samplerate=sr, channels=1, callback=audio_callback)\n",
    "stream.start()\n",
    "\n",
    "print(\"[INFO] Press 'q' to quit...\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(60,60))\n",
    "\n",
    "    face_label = \"No Face\"\n",
    "    face_confidence = 0.0\n",
    "    blink_rate = 0\n",
    "    blur_val = 0\n",
    "    texture_val = 0\n",
    "    brightness_val = 0\n",
    "\n",
    "    for (x, y, w, h) in faces:\n",
    "        face_crop = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # ------------- Face Liveness ----------------\n",
    "        try:\n",
    "            resized = cv2.resize(face_crop, (IMG_SIZE, IMG_SIZE))\n",
    "            face_input = img_to_array(cv2.cvtColor(resized, cv2.COLOR_BGR2RGB))/255.0\n",
    "            face_input = np.expand_dims(face_input, axis=0)\n",
    "            prediction = face_model.predict(face_input, verbose=0)[0][0]\n",
    "\n",
    "            texture_val = calculate_texture(face_crop)\n",
    "            blur_val = calculate_blur(face_crop)\n",
    "            brightness_val = calculate_brightness(face_crop)\n",
    "\n",
    "            # ------------- Eye Blink Detection ------------\n",
    "            eyes = eye_cascade.detectMultiScale(cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY))\n",
    "            ear = 1.0\n",
    "            if len(eyes) >= 2:\n",
    "                eye_pts = []\n",
    "                for ex, ey, ew, eh in eyes[:2]:\n",
    "                    eye_pts.append((ex, ey))\n",
    "                    eye_pts.append((ex+ew, ey+eh))\n",
    "                ear = eye_aspect_ratio(eye_pts)\n",
    "\n",
    "            if ear < EAR_THRESHOLD:\n",
    "                blink_counter += 1\n",
    "            else:\n",
    "                if blink_counter >= CONSEC_FRAMES:\n",
    "                    total_blinks += 1\n",
    "                blink_counter = 0\n",
    "\n",
    "            elapsed = time.time() - start_time\n",
    "            blink_rate = total_blinks / (elapsed / 60) if elapsed > 0 else 0\n",
    "\n",
    "            # ------------- Final Face Decision -------------\n",
    "            is_real = prediction > 0.5 and blink_rate >= 1 and texture_val > TEXTURE_THRESHOLD\n",
    "            face_label = \"Real\" if is_real else \"Fake\"\n",
    "            face_confidence = prediction * 100\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"[Face Error]\", e)\n",
    "\n",
    "        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0) if face_label==\"Real\" else (0,0,255), 2)\n",
    "\n",
    "    # ---------------- Audio Processing ----------------\n",
    "    if not q_audio.empty():\n",
    "        audio_chunk = q_audio.get().flatten()\n",
    "        audio_buffer.extend(audio_chunk.tolist())\n",
    "\n",
    "    if len(audio_buffer) >= sr:\n",
    "        audio_np = np.array(audio_buffer[:sr])\n",
    "        audio_buffer = audio_buffer[sr:]\n",
    "        mfcc = extract_mfcc(audio_np, sr)\n",
    "        if mfcc is not None:\n",
    "            try:\n",
    "                prediction = voice_model.predict_proba(mfcc)[0][1]  # probability of Real\n",
    "                voice_label = \"Real\" if prediction > VOICE_CONFIDENCE_THRESHOLD else \"Fake\"\n",
    "                voice_confidence = prediction * 100\n",
    "            except Exception as e:\n",
    "                voice_label = \"Model Error\"\n",
    "                voice_confidence = 0.0\n",
    "                print(\"[Voice Model Error]\", e)\n",
    "        else:\n",
    "            voice_label = \"Silent\"\n",
    "            voice_confidence = 0.0\n",
    "\n",
    "    # ---------------- Display Info ----------------\n",
    "    cv2.putText(frame, f\"Face: {face_label} ({face_confidence:.1f}%)\", (10,30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0) if face_label==\"Real\" else (0,0,255), 2)\n",
    "    cv2.putText(frame, f\"Voice: {voice_label} ({voice_confidence:.1f}%)\", (10,60),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,255,0) if voice_label==\"Real\" else (0,0,255), 2)\n",
    "    cv2.putText(frame, f\"Blinks: {total_blinks}\", (10,90), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 1)\n",
    "    cv2.putText(frame, f\"Blink Rate: {blink_rate:.2f}/min\", (10,110), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 1)\n",
    "    cv2.putText(frame, f\"Blur: {blur_val:.2f}\", (10,130), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 1)\n",
    "    cv2.putText(frame, f\"Texture: {texture_val:.2f}\", (10,150), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,0), 1)\n",
    "    cv2.putText(frame, f\"Brightness: {brightness_val:.2f}\", (10,170), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,255), 1)\n",
    "\n",
    "    cv2.imshow(\"Face & Voice Liveness Detection\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "stream.stop()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20718779-23a5-4e03-9d1c-714d0958fb90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
